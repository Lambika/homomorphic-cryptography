{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1: Training and Evaluation of Logistic Regression on Encrypted Data\n",
    "\n",
    "Welcome to this first use case tutorial, where we are going to show how to use TenSEAL for training and evaluating a logistic regression (LR) model on encrypted data (using homomorphic encryption) for heart disease prediction! If you haven't played with TenSEAL before, or need a quick overview of what homomorphic encryption is, I would suggest going through ['Tutorial 0 - Getting Started'](./Tutorial%200%20-%20Getting%20Started.ipynb) first.\n",
    "\n",
    "\n",
    "**Important note:** The goal of this tutorial isn't to show how efficient logistic regression is for this task, we will just go with whatever accuracy we get, but the training and evaluation on encrypted data should be comparable to when we use plain data.\n",
    "\n",
    "\n",
    "Authors:\n",
    "- Ayoub Benaissa - Twitter: [@y0uben11](https://twitter.com/y0uben11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: typing_extensions in c:\\users\\lambika\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (4.13.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade typing_extensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\lambika\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\lambika\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\lambika\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\lambika\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\lambika\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lambika\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\lambika\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (2023.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\lambika\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lambika\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "All modules are imported here. Make sure everything is installed by running the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tenseal as ts\n",
    "import pandas as pd\n",
    "import random\n",
    "from time import time\n",
    "\n",
    "# those are optional and are not necessary for training\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now prepare the training and test data. The dataset was downloaded from Kaggle [here](https://www.kaggle.com/dileep070/heart-disease-prediction-using-logistic-regression). This dataset includes patients' information along with a 10-year risk of future coronary heart disease (CHD) as a label. The goal is to build a model that can predict this 10-year CHD risk based on patients' information. You can read more about the dataset in the link provided. \n",
    "\n",
    "Alternatively, we also provide the `random_data()` function below that generates random, linearly separable points. You can use it instead of the dataset from Kaggle, for those who just want to see how things work. The rest of the tutorial should work in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# Data summary #############\n",
      "x_train has shape: torch.Size([780, 9])\n",
      "y_train has shape: torch.Size([780, 1])\n",
      "x_test has shape: torch.Size([334, 9])\n",
      "y_test has shape: torch.Size([334, 1])\n",
      "#######################################\n"
     ]
    }
   ],
   "source": [
    "torch.random.manual_seed(73)\n",
    "random.seed(73)\n",
    "\n",
    "\n",
    "def split_train_test(x, y, test_ratio=0.3):\n",
    "    idxs = [i for i in range(len(x))]\n",
    "    random.shuffle(idxs)\n",
    "    # delimiter between test and train data\n",
    "    delim = int(len(x) * test_ratio)\n",
    "    test_idxs, train_idxs = idxs[:delim], idxs[delim:]\n",
    "    return x[train_idxs], y[train_idxs], x[test_idxs], y[test_idxs]\n",
    "\n",
    "\n",
    "def heart_disease_data():\n",
    "    data = pd.read_csv(\"./data/framingham.csv\")\n",
    "    # drop rows with missing values\n",
    "    data = data.dropna()\n",
    "    # drop some features\n",
    "    data = data.drop(columns=[\"education\", \"currentSmoker\", \"BPMeds\", \"diabetes\", \"diaBP\", \"BMI\"])\n",
    "    # balance data\n",
    "    grouped = data.groupby('TenYearCHD')\n",
    "    data = grouped.apply(lambda x: x.sample(grouped.size().min(), random_state=73).reset_index(drop=True))\n",
    "    # extract labels\n",
    "    y = torch.tensor(data[\"TenYearCHD\"].values).float().unsqueeze(1)\n",
    "    data = data.drop(columns=[\"TenYearCHD\"])\n",
    "    # standardize data\n",
    "    data = (data - data.mean()) / data.std()\n",
    "    x = torch.tensor(data.values).float()\n",
    "    return split_train_test(x, y)\n",
    "\n",
    "\n",
    "def random_data(m=1024, n=2):\n",
    "    # data separable by the line `y = x`\n",
    "    x_train = torch.randn(m, n)\n",
    "    x_test = torch.randn(m // 2, n)\n",
    "    y_train = (x_train[:, 0] >= x_train[:, 1]).float().unsqueeze(0).t()\n",
    "    y_test = (x_test[:, 0] >= x_test[:, 1]).float().unsqueeze(0).t()\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "# You can use whatever data you want without modification to the tutorial\n",
    "# x_train, y_train, x_test, y_test = random_data()\n",
    "x_train, y_train, x_test, y_test = heart_disease_data()\n",
    "\n",
    "print(\"############# Data summary #############\")\n",
    "print(f\"x_train has shape: {x_train.shape}\")\n",
    "print(f\"y_train has shape: {y_train.shape}\")\n",
    "print(f\"x_test has shape: {x_test.shape}\")\n",
    "print(f\"y_test has shape: {y_test.shape}\")\n",
    "print(\"#######################################\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Logistic Regression Model\n",
    "\n",
    "We will start by training a logistic regression model (without any encryption), which can be viewed as a single layer neural network with a single node. We will be using this model as a means of comparison against encrypted training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_features):\n",
    "        super(LR, self).__init__()\n",
    "        self.lr = torch.nn.Linear(n_features, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = torch.sigmoid(self.lr(x))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N features shape 9\n"
     ]
    }
   ],
   "source": [
    "n_features = x_train.shape[1]\n",
    "print(f'N features shape {n_features}')\n",
    "model = LR(n_features)\n",
    "# use gradient descent with a learning_rate=1\n",
    "optim = torch.optim.SGD(model.parameters(), lr=1)\n",
    "# use Binary Cross Entropy Loss\n",
    "criterion = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 1: 0.8504331707954407\n",
      "Loss at epoch 2: 0.6863384246826172\n",
      "Loss at epoch 3: 0.6358115077018738\n",
      "Loss at epoch 4: 0.6193529367446899\n",
      "Loss at epoch 5: 0.6124349236488342\n"
     ]
    }
   ],
   "source": [
    "# define the number of epochs for both plain and encrypted training\n",
    "EPOCHS = 5\n",
    "\n",
    "def train(model, optim, criterion, x, y, epochs=EPOCHS):\n",
    "    for e in range(1, epochs + 1):\n",
    "        optim.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        print(f\"Loss at epoch {e}: {loss.data}\")\n",
    "    return model\n",
    "\n",
    "model = train(model, optim, criterion, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct value tensor([[False],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True]])\n",
      "Accuracy on plain test_set: 0.703592836856842\n"
     ]
    }
   ],
   "source": [
    "def accuracy(model, x, y):\n",
    "    out = model(x)\n",
    "    correct = torch.abs(y - out) < 0.5\n",
    "    print(f'Correct value {correct  }')\n",
    "    return correct.float().mean()\n",
    "\n",
    "plain_accuracy = accuracy(model, x_test, y_test)\n",
    "print(f\"Accuracy on plain test_set: {plain_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth to remember that a high accuracy isn't our goal. We just want to see that training on encrypted data doesn't affect the final result, so we will be comparing accuracies over encrypted data against the `plain_accuracy` we got here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encrypted Evaluation\n",
    "\n",
    "In this part, we will just focus on evaluating the logistic regression model with plain parameters (optionally encrypted parameters) on the encrypted test set. We first create a PyTorch-like LR model that can evaluate encrypted data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncryptedLR:\n",
    "    \n",
    "    def __init__(self, torch_lr):\n",
    "        # TenSEAL processes lists and not torch tensors,\n",
    "        # so we take out the parameters from the PyTorch model\n",
    "        self.weight = torch_lr.lr.weight.data.tolist()[0]\n",
    "        self.bias = torch_lr.lr.bias.data.tolist()\n",
    "        \n",
    "    def forward(self, enc_x):\n",
    "        # We don't need to perform sigmoid as this model\n",
    "        # will only be used for evaluation, and the label\n",
    "        # can be deduced without applying sigmoid\n",
    "        enc_out = enc_x.dot(self.weight) + self.bias\n",
    "        return enc_out\n",
    "    \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.forward(*args, **kwargs)\n",
    "        \n",
    "    ################################################\n",
    "    ## You can use the functions below to perform ##\n",
    "    ## the evaluation with an encrypted model     ##\n",
    "    ################################################\n",
    "    \n",
    "    def encrypt(self, context):\n",
    "        self.weight = ts.ckks_vector(context, self.weight)\n",
    "        self.bias = ts.ckks_vector(context, self.bias)\n",
    "        \n",
    "    def decrypt(self, context):\n",
    "        self.weight = self.weight.decrypt()\n",
    "        self.bias = self.bias.decrypt()\n",
    "        \n",
    "\n",
    "eelr = EncryptedLR(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a TenSEALContext for specifying the scheme and the parameters we are going to use. Here we choose small and secure parameters that allow us to make a single multiplication. That's enough for evaluating a logistic regression model, however, we will see that we need larger parameters when doing training on encrypted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "poly_mod_degree = 4096\n",
    "coeff_mod_bit_sizes = [40, 20, 40]\n",
    "# create TenSEALContext\n",
    "ctx_eval = ts.context(ts.SCHEME_TYPE.CKKS, poly_mod_degree, -1, coeff_mod_bit_sizes)\n",
    "# scale of ciphertext to use\n",
    "ctx_eval.global_scale = 2 ** 20\n",
    "# this key is needed for doing dot-product operations\n",
    "ctx_eval.generate_galois_keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will encrypt the whole test set before the evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0436, -0.7819, -0.7572,  ..., -0.5909, -0.1447, -0.7898],\n",
      "        [-0.9574, -0.3167, -0.7572,  ..., -1.3659, -0.0625,  0.0795],\n",
      "        [ 1.0436, -0.5493, -0.7572,  ..., -0.8969, -2.2818, -0.2620],\n",
      "        ...,\n",
      "        [ 1.0436, -0.0841,  1.6971,  ..., -0.4278,  0.7595, -0.4483],\n",
      "        [-0.9574,  0.8465,  0.0609,  ...,  1.5505,  3.6364,  0.0174],\n",
      "        [-0.9574, -1.4798, -0.3482,  ...,  0.0209, -0.7201, -0.4483]])\n",
      "[<tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF690D0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFAFA60>, <tenseal.tensors.ckksvector.CKKSVector object at 0x0000025715F124C0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002571D766820>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002571C41FD90>, <tenseal.tensors.ckksvector.CKKSVector object at 0x0000025715F28FD0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF5C250>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF5C3A0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF5C5B0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF5C130>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF5C1C0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF5C4F0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF5CD90>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF5CD60>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF5CEB0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF5C9A0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF5C2B0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF5CDC0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF5CCA0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF5C3D0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF5CF40>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF5C700>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF5C760>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF5CC40>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF5C940>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF5CAC0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF5CB20>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF5CD30>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF5C640>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF5C0A0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF5CA90>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF5CA30>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF5C430>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002571D7B3B20>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002571D7B3220>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002571D7B31C0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002571D7B3AF0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002571D7B32E0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002571D7B3520>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002571D7BFA90>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002571D7BFA00>, <tenseal.tensors.ckksvector.CKKSVector object at 0x0000025715ED6D60>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002571C426DC0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002571D7B9670>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002571D7B9640>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002571D7B96A0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002571D7B9460>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002571D7B9580>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002571D7B9850>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002571C430040>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002571C430D90>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002571C430D60>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002571C4308B0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002571C4309D0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002571C430CD0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002571C4309A0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002571C430A60>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002571C430A90>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002571C430190>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002571C430A30>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002571C430D30>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002571C430820>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002571C430970>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002571C430C70>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002571C430C40>, <tenseal.tensors.ckksvector.CKKSVector object at 0x0000025715F190A0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x0000025715F19490>, <tenseal.tensors.ckksvector.CKKSVector object at 0x0000025715F19730>, <tenseal.tensors.ckksvector.CKKSVector object at 0x0000025715F19880>, <tenseal.tensors.ckksvector.CKKSVector object at 0x0000025715F19850>, <tenseal.tensors.ckksvector.CKKSVector object at 0x0000025715F193D0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x0000025715F19910>, <tenseal.tensors.ckksvector.CKKSVector object at 0x0000025715F196D0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x0000025715F19790>, <tenseal.tensors.ckksvector.CKKSVector object at 0x0000025715F19610>, <tenseal.tensors.ckksvector.CKKSVector object at 0x0000025715F196A0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x0000025715EF16A0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x0000025715EF1310>, <tenseal.tensors.ckksvector.CKKSVector object at 0x0000025715EF17F0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x0000025715EF1460>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002570A888100>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002571C42CB20>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002571259BAC0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x00000257738D5B80>, <tenseal.tensors.ckksvector.CKKSVector object at 0x0000025715F04A60>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002570A896880>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002570A896940>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002570A8961C0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002570A896640>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF3DB20>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF3DC70>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF3DA90>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF3DF10>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF3DD60>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF3DDC0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF3DE50>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF3DF70>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF3DE80>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF3DA60>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577F02B070>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577F02B0D0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577F02B130>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577F02B190>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577F02B1F0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577F02B250>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577F02B2B0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577F02B310>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577F02B370>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577F02B3D0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577F02B430>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577F02B490>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577F02B4F0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577F02B550>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577F02B5B0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577F02B610>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577F02B670>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577F02B6D0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577F02B730>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577F02B790>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577F02B7F0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577F02B850>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577F02B8B0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577F02B910>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577F02B970>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577F02B9D0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577F02BA30>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577F02BA90>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577F02BAF0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577F02BB50>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577F02BBB0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577F02BC10>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577F02BC70>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577F02BCD0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577F02BD30>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577F02BD90>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577F02BDF0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577F02BE50>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577F02BEB0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577F02BF10>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577F02BF70>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577F02BFA0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002571D78E130>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002571D78EAF0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF64040>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF640A0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF64100>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF64160>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF641C0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF64220>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF64280>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF642E0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF64340>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF643A0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF64400>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF64460>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF644C0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF64520>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF64580>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF645E0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF64640>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF646A0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF64700>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF64760>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF647C0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF64820>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF64880>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF648E0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF64940>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF649A0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF64A00>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF64A60>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF64AC0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF64B20>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF64B80>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF64BE0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF64C40>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF64CA0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF64D00>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF64D60>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF64DC0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF64E20>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF64E80>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF64EE0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF64F40>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF64FA0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF67040>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF670A0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF67100>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF67160>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF671C0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF67220>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF67280>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF672E0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF67340>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF673A0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF67400>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF67460>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF674C0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF67520>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF67580>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF675E0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF67640>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF676A0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF67700>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF67760>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF677C0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF67820>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF67880>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF678E0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF67940>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF679A0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF67A00>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF67A60>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF67AC0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF67B20>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF67B80>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF67BE0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF67C40>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF67CA0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF67D00>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF67D60>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF67DC0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF67E20>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF67E80>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF67EE0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF67F40>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF67FA0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE9040>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE90A0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE9100>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE9160>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE91C0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE9220>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE9280>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE92E0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE9340>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE93A0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE9400>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE9460>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE94C0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE9520>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE9580>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE95E0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE9640>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE96A0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE9700>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE9760>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE97C0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE9820>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE9880>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE98E0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE9940>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE99A0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE9A00>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE9A60>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE9AC0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE9B20>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE9B80>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE9BE0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE9C40>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE9CA0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE9D00>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE9D60>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE9DC0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE9E20>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE9E80>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE9EE0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE9F40>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE9FA0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE3040>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE30A0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE3100>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE3160>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE31C0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE3220>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE3280>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE32E0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE3340>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE33A0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE3400>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE3460>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE34C0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE3520>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE3580>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE35E0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE3640>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE36A0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE3700>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE3760>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE37C0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE3820>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE3880>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE38E0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE3940>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE39A0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE3A00>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE3A60>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE3AC0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE3B20>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE3B80>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE3BE0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE3C40>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE3CA0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE3D00>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE3D60>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE3DC0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE3E20>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE3E80>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE3EE0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE3F40>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EFE3FA0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF8D040>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF8D0A0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF8D100>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF8D160>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF8D1C0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF8D220>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF8D280>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF8D2E0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF8D340>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF8D3A0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF8D400>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF8D460>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF8D4C0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF8D520>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF8D580>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF8D5E0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF8D640>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF8D6A0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF8D700>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF8D760>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF8D7C0>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF8D820>, <tenseal.tensors.ckksvector.CKKSVector object at 0x000002577EF8D880>]\n",
      "Encryption of the test-set took 0 seconds\n"
     ]
    }
   ],
   "source": [
    "t_start = time()\n",
    "print(x_test)\n",
    "enc_x_test = [ts.ckks_vector(ctx_eval, x.tolist()) for x in x_test]\n",
    "print(enc_x_test)\n",
    "\n",
    "t_end = time()\n",
    "print(f\"Encryption of the test-set took {int(t_end - t_start)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (optional) encrypt the model's parameters\n",
    "eelr.encrypt(ctx_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may have already noticed when we built the EncryptedLR class, we don't compute the sigmoid function on the encrypted output of the linear layer, simply because it's not needed, and computing sigmoid over encrypted data will increase the computation time and require larger encryption parameters. However, we will use sigmoid for the encrypted training part. We now proceed with the evaluation of the encrypted test set and compare the accuracy to the one on the plain test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decrypted value [-0.9084274408362748]\n",
      "Decrypted value [-1.3018855006452248]\n",
      "Decrypted value [-1.1069876141301174]\n",
      "Decrypted value [-0.2211070551392808]\n",
      "Decrypted value [-0.2015788941429658]\n",
      "Decrypted value [-0.17051445944424726]\n",
      "Decrypted value [0.5438873607265882]\n",
      "Decrypted value [-1.8605691924205254]\n",
      "Decrypted value [-1.1423454740672234]\n",
      "Decrypted value [-1.231044061060436]\n",
      "Decrypted value [-0.12360778033535819]\n",
      "Decrypted value [-0.6746777406485672]\n",
      "Decrypted value [0.09120585257133411]\n",
      "Decrypted value [-0.1880567573645662]\n",
      "Decrypted value [0.2262511024526777]\n",
      "Decrypted value [0.5843117321406589]\n",
      "Decrypted value [0.7484345721501486]\n",
      "Decrypted value [-0.027375224330006075]\n",
      "Decrypted value [0.7965648979083536]\n",
      "Decrypted value [-1.3651631824164927]\n",
      "Decrypted value [0.009859786818236842]\n",
      "Decrypted value [-0.8730524322569911]\n",
      "Decrypted value [0.010046171428266033]\n",
      "Decrypted value [0.05553093352615662]\n",
      "Decrypted value [0.2448323702771581]\n",
      "Decrypted value [-1.2434743076724148]\n",
      "Decrypted value [-0.9940309439443371]\n",
      "Decrypted value [1.0999091016269065]\n",
      "Decrypted value [-0.06858819138093425]\n",
      "Decrypted value [0.16056272510158184]\n",
      "Decrypted value [0.1939753659290104]\n",
      "Decrypted value [-1.119003122566135]\n",
      "Decrypted value [-0.3196789506909759]\n",
      "Decrypted value [-0.3638979224639507]\n",
      "Decrypted value [0.06030577495550894]\n",
      "Decrypted value [-0.5481166864398777]\n",
      "Decrypted value [-0.43286444076699565]\n",
      "Decrypted value [-0.9440983146592139]\n",
      "Decrypted value [-0.18857267063806532]\n",
      "Decrypted value [-0.4884985488450042]\n",
      "Decrypted value [0.9308278401613987]\n",
      "Decrypted value [0.5133673270505337]\n",
      "Decrypted value [0.3881421462626191]\n",
      "Decrypted value [-0.9768346202383227]\n",
      "Decrypted value [-0.5478620012483477]\n",
      "Decrypted value [0.134994085686233]\n",
      "Decrypted value [-0.8764479817376192]\n",
      "Decrypted value [1.5961790663864488]\n",
      "Decrypted value [0.6476827504219942]\n",
      "Decrypted value [0.1868483799699888]\n",
      "Decrypted value [-0.7021469362046702]\n",
      "Decrypted value [-0.1141076281376584]\n",
      "Decrypted value [0.30832711389448736]\n",
      "Decrypted value [-0.2336249119181466]\n",
      "Decrypted value [-1.4301915562308543]\n",
      "Decrypted value [3.124076662920887]\n",
      "Decrypted value [-1.0655676412650767]\n",
      "Decrypted value [-0.46116689492469665]\n",
      "Decrypted value [-0.5199047084222167]\n",
      "Decrypted value [0.5368604928390074]\n",
      "Decrypted value [-2.0758877267194547]\n",
      "Decrypted value [-0.719617360288729]\n",
      "Decrypted value [-1.2349188221416483]\n",
      "Decrypted value [0.11125185338945712]\n",
      "Decrypted value [-0.9231037597026029]\n",
      "Decrypted value [0.5175139725386333]\n",
      "Decrypted value [1.3473107137146116]\n",
      "Decrypted value [0.3429072021416072]\n",
      "Decrypted value [-0.017967471150883166]\n",
      "Decrypted value [-0.574298169507468]\n",
      "Decrypted value [0.3665350539659964]\n",
      "Decrypted value [-0.27653718894870005]\n",
      "Decrypted value [-0.3446728204631361]\n",
      "Decrypted value [-1.2602404605104112]\n",
      "Decrypted value [-0.006136973473058804]\n",
      "Decrypted value [-0.22974500879854143]\n",
      "Decrypted value [0.5293057727162852]\n",
      "Decrypted value [-0.30365497248474743]\n",
      "Decrypted value [-0.4524071031434652]\n",
      "Decrypted value [0.5967440665386804]\n",
      "Decrypted value [-1.027365041365298]\n",
      "Decrypted value [0.8346846469499962]\n",
      "Decrypted value [0.22015723736648435]\n",
      "Decrypted value [0.09395322213955466]\n",
      "Decrypted value [-0.3626064191043861]\n",
      "Decrypted value [1.1013773651948675]\n",
      "Decrypted value [0.09869108227894499]\n",
      "Decrypted value [0.45560261638036925]\n",
      "Decrypted value [0.09754794574067088]\n",
      "Decrypted value [-1.1803171137609634]\n",
      "Decrypted value [1.5293299485704184]\n",
      "Decrypted value [-0.4693190435865109]\n",
      "Decrypted value [-0.9083222410095272]\n",
      "Decrypted value [-0.4573433437643992]\n",
      "Decrypted value [-1.0685239064060081]\n",
      "Decrypted value [0.2715225872770407]\n",
      "Decrypted value [0.5098686395568865]\n",
      "Decrypted value [-0.49444938930768406]\n",
      "Decrypted value [-0.3564807239589463]\n",
      "Decrypted value [-0.06388524532725628]\n",
      "Decrypted value [1.1044202242047276]\n",
      "Decrypted value [1.043349251193093]\n",
      "Decrypted value [-0.8222674017516941]\n",
      "Decrypted value [-1.4062338060202293]\n",
      "Decrypted value [-0.4537297769794928]\n",
      "Decrypted value [-0.4803892742281413]\n",
      "Decrypted value [0.2410209799873213]\n",
      "Decrypted value [-0.37912861887773247]\n",
      "Decrypted value [-0.5667307736035394]\n",
      "Decrypted value [0.13772145388098067]\n",
      "Decrypted value [0.7038637388984932]\n",
      "Decrypted value [-0.28401919281139204]\n",
      "Decrypted value [-1.004572937283577]\n",
      "Decrypted value [-0.35866137655676655]\n",
      "Decrypted value [-0.6480255730869272]\n",
      "Decrypted value [-0.778734161164909]\n",
      "Decrypted value [-0.33304174072542153]\n",
      "Decrypted value [1.0675302707943002]\n",
      "Decrypted value [-0.23612968168196952]\n",
      "Decrypted value [-0.20763610060929505]\n",
      "Decrypted value [-1.633990340613083]\n",
      "Decrypted value [-1.342790643495732]\n",
      "Decrypted value [-1.3138496910026447]\n",
      "Decrypted value [0.8672197840192082]\n",
      "Decrypted value [1.00514353966944]\n",
      "Decrypted value [-0.5338394546849942]\n",
      "Decrypted value [-0.30846227141046795]\n",
      "Decrypted value [-0.12873067255643267]\n",
      "Decrypted value [-0.47750243189903885]\n",
      "Decrypted value [-0.7047186986634084]\n",
      "Decrypted value [-0.49474462436622496]\n",
      "Decrypted value [0.5780317716983461]\n",
      "Decrypted value [0.3334548332135781]\n",
      "Decrypted value [1.3248120292501426]\n",
      "Decrypted value [-0.27932538000720053]\n",
      "Decrypted value [0.06641318512573152]\n",
      "Decrypted value [-1.0906290574056872]\n",
      "Decrypted value [0.45707165895644875]\n",
      "Decrypted value [-1.164454599164577]\n",
      "Decrypted value [0.3289746680861852]\n",
      "Decrypted value [-0.4055419316974507]\n",
      "Decrypted value [-0.9360053320615467]\n",
      "Decrypted value [-0.8917632002479892]\n",
      "Decrypted value [0.44931705506994046]\n",
      "Decrypted value [-1.1969654247945651]\n",
      "Decrypted value [-1.3469424093804647]\n",
      "Decrypted value [-0.34612446067557295]\n",
      "Decrypted value [-0.35591182695899953]\n",
      "Decrypted value [-1.4025262408950105]\n",
      "Decrypted value [-1.1630363869087519]\n",
      "Decrypted value [0.9600843085833017]\n",
      "Decrypted value [-0.47576350881038965]\n",
      "Decrypted value [-0.78354214141854]\n",
      "Decrypted value [-1.5081278016666912]\n",
      "Decrypted value [0.2525135537655819]\n",
      "Decrypted value [0.35215107683029667]\n",
      "Decrypted value [-1.0143716298639782]\n",
      "Decrypted value [-0.766847328296951]\n",
      "Decrypted value [0.07568329756092983]\n",
      "Decrypted value [0.6744958167869595]\n",
      "Decrypted value [-0.6832354568674917]\n",
      "Decrypted value [1.0203212270842956]\n",
      "Decrypted value [-1.152795970323173]\n",
      "Decrypted value [-0.5878669122007043]\n",
      "Decrypted value [-1.3611246981115324]\n",
      "Decrypted value [0.8942827035424523]\n",
      "Decrypted value [-1.3735995174358164]\n",
      "Decrypted value [-0.45241882110684506]\n",
      "Decrypted value [-0.27228805289930236]\n",
      "Decrypted value [0.7561297191138935]\n",
      "Decrypted value [-0.3403240759959292]\n",
      "Decrypted value [-1.0014758432809359]\n",
      "Decrypted value [-0.3224310643954274]\n",
      "Decrypted value [1.0879003102219595]\n",
      "Decrypted value [0.5137047117254131]\n",
      "Decrypted value [0.1716583866727175]\n",
      "Decrypted value [-0.5861521156971004]\n",
      "Decrypted value [1.352985761621575]\n",
      "Decrypted value [-0.7491885065824728]\n",
      "Decrypted value [-0.20667724261081127]\n",
      "Decrypted value [0.4121177598639063]\n",
      "Decrypted value [-0.02072620740995469]\n",
      "Decrypted value [-0.6263333042061199]\n",
      "Decrypted value [-0.3326083360256132]\n",
      "Decrypted value [0.1629719055175215]\n",
      "Decrypted value [1.0333467720477763]\n",
      "Decrypted value [0.2914293667570269]\n",
      "Decrypted value [-0.4978101531386986]\n",
      "Decrypted value [0.3290056172331341]\n",
      "Decrypted value [-0.57983900152068]\n",
      "Decrypted value [0.2607604275877293]\n",
      "Decrypted value [-0.412689915325143]\n",
      "Decrypted value [-0.5833792337380319]\n",
      "Decrypted value [-0.7550094000979021]\n",
      "Decrypted value [0.6157624016794279]\n",
      "Decrypted value [-0.3814328717594263]\n",
      "Decrypted value [0.8219752162905125]\n",
      "Decrypted value [-0.5373952816416433]\n",
      "Decrypted value [-0.5070589200843152]\n",
      "Decrypted value [0.04019062916694652]\n",
      "Decrypted value [-1.2669292024186314]\n",
      "Decrypted value [-1.097890122477635]\n",
      "Decrypted value [-0.554436054018738]\n",
      "Decrypted value [-0.11460017473436325]\n",
      "Decrypted value [-0.8942353874390057]\n",
      "Decrypted value [-0.8339664409966887]\n",
      "Decrypted value [1.6383180485994775]\n",
      "Decrypted value [0.748752901536019]\n",
      "Decrypted value [-1.4004833961508731]\n",
      "Decrypted value [-0.11354782270310655]\n",
      "Decrypted value [1.2786049676215605]\n",
      "Decrypted value [-1.6505041997654242]\n",
      "Decrypted value [-1.5735915574163957]\n",
      "Decrypted value [0.35563442112400423]\n",
      "Decrypted value [-0.3214666255248792]\n",
      "Decrypted value [-1.022405786898663]\n",
      "Decrypted value [-0.5257040500844474]\n",
      "Decrypted value [0.46487756510228073]\n",
      "Decrypted value [-0.6633925268490652]\n",
      "Decrypted value [0.08756078940089743]\n",
      "Decrypted value [-0.5081806072275562]\n",
      "Decrypted value [-1.4740430095065475]\n",
      "Decrypted value [-0.5791080832810983]\n",
      "Decrypted value [-0.38103680556236824]\n",
      "Decrypted value [-1.2263813242525883]\n",
      "Decrypted value [1.4800367000688825]\n",
      "Decrypted value [-0.4886446212969078]\n",
      "Decrypted value [-0.18996089124573534]\n",
      "Decrypted value [0.964202861190022]\n",
      "Decrypted value [0.05016482527400263]\n",
      "Decrypted value [-1.3900440756972177]\n",
      "Decrypted value [-0.11267447484122652]\n",
      "Decrypted value [1.5852950222301636]\n",
      "Decrypted value [-0.5135698545554518]\n",
      "Decrypted value [1.395602764480584]\n",
      "Decrypted value [0.00996629837723742]\n",
      "Decrypted value [-0.333089473878763]\n",
      "Decrypted value [-0.04085607027499537]\n",
      "Decrypted value [3.0910456066773286]\n",
      "Decrypted value [0.0931991508451071]\n",
      "Decrypted value [-0.19228473668727458]\n",
      "Decrypted value [-0.20234390675126554]\n",
      "Decrypted value [0.849805611707321]\n",
      "Decrypted value [0.33075923247170125]\n",
      "Decrypted value [0.8135131504210833]\n",
      "Decrypted value [1.3798738468560419]\n",
      "Decrypted value [-1.8410488152813562]\n",
      "Decrypted value [-0.6202056178897344]\n",
      "Decrypted value [0.14103175294562598]\n",
      "Decrypted value [2.818657178906267]\n",
      "Decrypted value [-0.49570337849771623]\n",
      "Decrypted value [-0.8731836454340299]\n",
      "Decrypted value [0.6074797580790975]\n",
      "Decrypted value [-1.1152991887581931]\n",
      "Decrypted value [0.5686171451253017]\n",
      "Decrypted value [-0.09737955022802797]\n",
      "Decrypted value [0.20398544135732738]\n",
      "Decrypted value [-0.35213299507321866]\n",
      "Decrypted value [-0.6147949051456095]\n",
      "Decrypted value [-0.6178735704661861]\n",
      "Decrypted value [0.9522314895677053]\n",
      "Decrypted value [-0.2230006761925291]\n",
      "Decrypted value [1.3052269637707623]\n",
      "Decrypted value [0.8819799376028512]\n",
      "Decrypted value [-0.27727675697317383]\n",
      "Decrypted value [-0.9595309062386455]\n",
      "Decrypted value [-0.33275252272515626]\n",
      "Decrypted value [0.22857784337075054]\n",
      "Decrypted value [-1.2251825465881896]\n",
      "Decrypted value [0.4455110818515056]\n",
      "Decrypted value [-0.0853083442103517]\n",
      "Decrypted value [-0.4534299736367567]\n",
      "Decrypted value [-1.8521763833653686]\n",
      "Decrypted value [-0.24888727526324597]\n",
      "Decrypted value [-0.38383907314629107]\n",
      "Decrypted value [-1.0471544518234412]\n",
      "Decrypted value [-0.8168844499637925]\n",
      "Decrypted value [0.6893568742201239]\n",
      "Decrypted value [-1.1737119054695064]\n",
      "Decrypted value [-0.3265758307952115]\n",
      "Decrypted value [-0.6585414169308926]\n",
      "Decrypted value [-0.053522693693083954]\n",
      "Decrypted value [-0.4622598256239962]\n",
      "Decrypted value [-0.9544466727725156]\n",
      "Decrypted value [-0.005635864635985671]\n",
      "Decrypted value [-1.0104516677576805]\n",
      "Decrypted value [-0.06042815028567047]\n",
      "Decrypted value [-1.3625334420341622]\n",
      "Decrypted value [-0.46126422732228284]\n",
      "Decrypted value [-0.6030983445679889]\n",
      "Decrypted value [-0.2717821235776155]\n",
      "Decrypted value [-0.19876817420526074]\n",
      "Decrypted value [0.46178820643874713]\n",
      "Decrypted value [0.07105168120628652]\n",
      "Decrypted value [-0.7295225095650149]\n",
      "Decrypted value [-0.16950075367997608]\n",
      "Decrypted value [-0.35997887308907006]\n",
      "Decrypted value [-0.031729296433096896]\n",
      "Decrypted value [-0.6222136778667862]\n",
      "Decrypted value [-0.31380708064938423]\n",
      "Decrypted value [-0.5519015561280388]\n",
      "Decrypted value [-0.6557114361063656]\n",
      "Decrypted value [-0.27910018240063217]\n",
      "Decrypted value [-0.32372419717786816]\n",
      "Decrypted value [-0.682598288737885]\n",
      "Decrypted value [-0.43235553539956845]\n",
      "Decrypted value [0.03334479994906492]\n",
      "Decrypted value [-0.223969587924724]\n",
      "Decrypted value [-0.03988104937917691]\n",
      "Decrypted value [0.0005090442457822211]\n",
      "Decrypted value [-0.08488071405247857]\n",
      "Decrypted value [1.5087070437908152]\n",
      "Decrypted value [-0.6858423534494984]\n",
      "Decrypted value [0.025451488917040063]\n",
      "Decrypted value [0.09388011440296641]\n",
      "Decrypted value [-1.0651751193984]\n",
      "Decrypted value [-0.6311992119876787]\n",
      "Decrypted value [0.5246546020568083]\n",
      "Decrypted value [-0.16054699670638256]\n",
      "Decrypted value [0.25138428173438343]\n",
      "Decrypted value [-0.16403085823909436]\n",
      "Decrypted value [-1.0444304870916916]\n",
      "Decrypted value [-1.6221455894194445]\n",
      "Decrypted value [-0.3438214212782333]\n",
      "Decrypted value [1.6545785514584248]\n",
      "Decrypted value [0.6261701870570472]\n",
      "Decrypted value [0.655484164595904]\n",
      "Decrypted value [0.2308687241595236]\n",
      "Decrypted value [-1.4853745502882991]\n",
      "Decrypted value [0.46796515554062135]\n",
      "Decrypted value [-0.4003594032237434]\n",
      "Decrypted value [0.10087443463536995]\n",
      "Decrypted value [1.0792984971148039]\n",
      "Decrypted value [-1.3538098450554434]\n",
      "Evaluated test_set of 334 entries in 1 seconds\n",
      "Accuracy: 228/334 = 0.6826347305389222\n",
      "Difference between plain and encrypted accuracies: 0.020958125591278076\n"
     ]
    }
   ],
   "source": [
    "def encrypted_evaluation(model, enc_x_test, y_test):\n",
    "    t_start = time()\n",
    "    \n",
    "    correct = 0\n",
    "    for enc_x, y in zip(enc_x_test, y_test):\n",
    "        # encrypted evaluation\n",
    "        enc_out = model(enc_x)\n",
    "        # plain comparison\n",
    "        out = enc_out.decrypt()\n",
    "        print(f\"Decrypted value {out}\")\n",
    "        out = torch.tensor(out)\n",
    "        out = torch.sigmoid(out)\n",
    "        if torch.abs(out - y) < 0.5:\n",
    "            correct += 1\n",
    "    \n",
    "    t_end = time()\n",
    "    print(f\"Evaluated test_set of {len(x_test)} entries in {int(t_end - t_start)} seconds\")\n",
    "    print(f\"Accuracy: {correct}/{len(x_test)} = {correct / len(x_test)}\")\n",
    "    return correct / len(x_test)\n",
    "    \n",
    "\n",
    "encrypted_accuracy = encrypted_evaluation(eelr, enc_x_test, y_test)\n",
    "diff_accuracy = plain_accuracy - encrypted_accuracy\n",
    "print(f\"Difference between plain and encrypted accuracies: {diff_accuracy}\")\n",
    "if diff_accuracy < 0:\n",
    "    print(\"Oh! We got a better accuracy on the encrypted test-set! The noise was on our side...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw that evaluating on the encrypted test set doesn't affect the accuracy that much. I've even seen examples where the encrypted evaluation performs better."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training an Encrypted Logistic Regression Model on Encrypted Data\n",
    "\n",
    "In this part we will redefine a PyTorch-like model that can both forward encrypted data, as well as backpropagate to update the weights and thus train the encrypted logistic regression model on encrypted data. Below are more details about the training.\n",
    "\n",
    "#### Loss Function\n",
    "\n",
    "We are using the binary cross entropy loss function with regularization (more about the why of regularization will follow) where $y^{(i)}$ is the i'th expected label, $\\hat{y}^{(i)}$ is the i'th output of the logistic regression model and $\\theta$ is our n-sized weight vector.\n",
    "\n",
    "$$Loss(\\theta) = - \\frac{1}{m} \\sum_{i=1}^m [y^{(i)} log(\\hat{y}^{(i)}) + (1 - y^{(i)}) log (1 - \\hat{y}^{(i)})] + \\frac{\\lambda}{2m} \\sum_{j=1}^n \\theta_j^2$$\n",
    "\n",
    "#### Parameters Update\n",
    "\n",
    "For updating the parameter, the usual rule is as follows, where $x^{(i)}$ is the i'th input data:\n",
    "\n",
    "$$\\theta_j = \\theta_j - \\alpha \\; [ \\frac{1}{m} \\sum_{i=1}^m (\\hat{y}^{(i)} - y^{(i)}) x^{(i)} + \\frac{\\lambda}{m} \\theta_j]$$\n",
    "\n",
    "However, due to homomorphic encryption constraint, we preferred to use an $\\alpha = 1$ to reduce a multiplication and set $\\frac{\\lambda}{m} = 0.05$ which gets us to the following update rule:\n",
    "\n",
    "$$\\theta_j = \\theta_j - [ \\frac{1}{m} \\sum_{i=1}^m (\\hat{y}^{(i)} - y^{(i)}) x^{(i)} + 0.05 \\theta_j]$$\n",
    "\n",
    "#### Sigmoid Approximation\n",
    "\n",
    "Since we can't simply compute sigmoid on encrypted data, we need to approximate it using a low degree polynomial, the lower the degree the better, as we aim to perform as few multiplications as possible, to be able to use smaller parameters and thus optimize computation. This tutorial uses a degree 3 polynomial from https://eprint.iacr.org/2018/462.pdf, which approximates the sigmoid function in the range $[-5,5]$.\n",
    "\n",
    "$$\\sigma(x) = 0.5 + 0.197 x - 0.004 x^3$$\n",
    "\n",
    "#### Homomorphic Encryption Parameters\n",
    "\n",
    "From the input data to the parameter update, a ciphertext will need a multiplicative depth of 6, 1 for the dot product operation, 2 for the sigmoid approximation, and 3 for the backpropagation phase (one is actually hidden in the `self._delta_w += enc_x * out_minus_y` operation in the `backward()` function, which is multiplying a 1-sized vector with an n-sized one, which requires masking the first slot and replicating it n times in the first vector). With a scale of around 20 bits, we need 6 coefficients modulus with the same bit-size as the scale, plus the last coefficient, which needs more bits, we are already out of the 4096 polynomial modulus degree (which requires < 109 total bit count of the coefficients modulus, if we consider 128-bit security), so we will use 8192. This will allow us to batch up to 4096 values in a single ciphertext, but we are far away from this limitation, so we shouldn't even think about it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncryptedLR:\n",
    "    \n",
    "    def __init__(self, torch_lr):\n",
    "        self.weight = torch_lr.lr.weight.data.tolist()[0]\n",
    "        self.bias = torch_lr.lr.bias.data.tolist()\n",
    "        # we accumulate gradients and counts the number of iterations\n",
    "        self._delta_w = 0\n",
    "        self._delta_b = 0\n",
    "        self._count = 0\n",
    "        \n",
    "    def forward(self, enc_x):\n",
    "        enc_out = enc_x.dot(self.weight) + self.bias\n",
    "        enc_out = EncryptedLR.sigmoid(enc_out)\n",
    "        return enc_out\n",
    "    \n",
    "    def backward(self, enc_x, enc_out, enc_y):\n",
    "        out_minus_y = (enc_out - enc_y)\n",
    "        self._delta_w += enc_x * out_minus_y\n",
    "        self._delta_b += out_minus_y\n",
    "        self._count += 1\n",
    "        \n",
    "    def update_parameters(self):\n",
    "        if self._count == 0:\n",
    "            raise RuntimeError(\"You should at least run one forward iteration\")\n",
    "        # update weights\n",
    "        # We use a small regularization term to keep the output\n",
    "        # of the linear layer in the range of the sigmoid approximation\n",
    "        self.weight -= self._delta_w * (1 / self._count) + self.weight * 0.05\n",
    "        self.bias -= self._delta_b * (1 / self._count)\n",
    "        # reset gradient accumulators and iterations count\n",
    "        self._delta_w = 0\n",
    "        self._delta_b = 0\n",
    "        self._count = 0\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(enc_x):\n",
    "        # We use the polynomial approximation of degree 3\n",
    "        # sigmoid(x) = 0.5 + 0.197 * x - 0.004 * x^3\n",
    "        # from https://eprint.iacr.org/2018/462.pdf\n",
    "        # which fits the function pretty well in the range [-5,5]\n",
    "        return enc_x.polyval([0.5, 0.197, 0, -0.004])\n",
    "    \n",
    "    def plain_accuracy(self, x_test, y_test):\n",
    "        # evaluate accuracy of the model on\n",
    "        # the plain (x_test, y_test) dataset\n",
    "        w = torch.tensor(self.weight)\n",
    "        b = torch.tensor(self.bias)\n",
    "        out = torch.sigmoid(x_test.matmul(w) + b).reshape(-1, 1)\n",
    "        correct = torch.abs(y_test - out) < 0.5\n",
    "        return correct.float().mean()    \n",
    "    \n",
    "    def encrypt(self, context):\n",
    "        self.weight = ts.ckks_vector(context, self.weight)\n",
    "        self.bias = ts.ckks_vector(context, self.bias)\n",
    "        \n",
    "    def decrypt(self):\n",
    "        self.weight = self.weight.decrypt()\n",
    "        self.bias = self.bias.decrypt()\n",
    "        \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.forward(*args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "poly_mod_degree = 8192\n",
    "coeff_mod_bit_sizes = [40, 21, 21, 21, 21, 21, 21, 40]\n",
    "# create TenSEALContext\n",
    "ctx_training = ts.context(ts.SCHEME_TYPE.CKKS, poly_mod_degree, -1, coeff_mod_bit_sizes)\n",
    "ctx_training.global_scale = 2 ** 21\n",
    "ctx_training.generate_galois_keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encryption of the training_set took 16 seconds\n"
     ]
    }
   ],
   "source": [
    "t_start = time()\n",
    "enc_x_train = [ts.ckks_vector(ctx_training, x.tolist()) for x in x_train]\n",
    "enc_y_train = [ts.ckks_vector(ctx_training, y.tolist()) for y in y_train]\n",
    "t_end = time()\n",
    "print(f\"Encryption of the training_set took {int(t_end - t_start)} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we study the distribution of `x.dot(weight) + bias` in both plain and encrypted domains. Making sure that it falls into the range $[-5,5]$, which is where our sigmoid approximation is good at, and we don't want to feed it data that is out of this range so that we don't get erroneous output, which can make our training behave unpredictably. But the weights will change during the training process, and we should try to keep them as small as possible while still learning. A technique often used with logistic regression, and we do exactly this (but serving another purpose which is *generalization*), is known as *regularization*, and you might already have spotted the additional term `self.weight * 0.05` in the `update_parameters()` function, which is the result of doing regularization.\n",
    "\n",
    "To recap, since our sigmoid approximation is only good in the range $[-5,5]$, we want to have all its inputs in that range. In order to do this, we need to keep our logistic regression parameters as small as possible, so we apply regularization.\n",
    "\n",
    "**Note:** Keeping the parameters small certainly reduces the magnitude of the output, but we can also get out of range if the data wasn't standardized. You may have spotted that we standardized the data with a mean of 0 and std of 1, this was both for better performance, as well as to keep the inputs to the sigmoid in the desired range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution on plain data:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution on encrypted data:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "normal_dist = lambda x, mean, var: np.exp(- np.square(x - mean) / (2 * var)) / np.sqrt(2 * np.pi * var)\n",
    "\n",
    "def plot_normal_dist(mean, var, rmin=-10, rmax=10):\n",
    "    x = np.arange(rmin, rmax, 0.01)\n",
    "    y = normal_dist(x, mean, var)\n",
    "    fig = plt.plot(x, y)\n",
    "    \n",
    "# plain distribution\n",
    "lr = LR(n_features)\n",
    "data = lr.lr(x_test)\n",
    "mean, var = map(float, [data.mean(), data.std() ** 2])\n",
    "plot_normal_dist(mean, var)\n",
    "print(\"Distribution on plain data:\")\n",
    "plt.show()\n",
    "\n",
    "# encrypted distribution\n",
    "def encrypted_out_distribution(eelr, enc_x_test):\n",
    "    w = eelr.weight\n",
    "    b = eelr.bias\n",
    "    data = []\n",
    "    for enc_x in enc_x_test:\n",
    "        enc_out = enc_x.dot(w) + b\n",
    "        data.append(enc_out.decrypt())\n",
    "    data = torch.tensor(data)\n",
    "    mean, var = map(float, [data.mean(), data.std() ** 2])\n",
    "    plot_normal_dist(mean, var)\n",
    "    print(\"Distribution on encrypted data:\")\n",
    "    plt.show()\n",
    "\n",
    "eelr = EncryptedLR(lr)\n",
    "eelr.encrypt(ctx_training)\n",
    "encrypted_out_distribution(eelr, enc_x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the data falls into $[-5,5]$, the sigmoid approximation should be good enough!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally reached the last part, which is about training an encrypted logistic regression model on encrypted data! You can see that we decrypt the weights and re-encrypt them again after every epoch, this is necessary since after updating the weights at the end of the epoch, we can no longer use them to perform enough multiplications, so we need to get them back to the initial ciphertext level. In a real scenario, this would translate to sending the weights back to the secret-key holder for decryption and re-encryption. In that case, it will result in just a few Kilobytes of communication per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at epoch #0 is 0.523952066898346\n",
      "Accuracy at epoch #1 is 0.6706587076187134\n",
      "Accuracy at epoch #2 is 0.688622772693634\n",
      "Accuracy at epoch #3 is 0.682634711265564\n",
      "Accuracy at epoch #4 is 0.658682644367218\n",
      "Accuracy at epoch #5 is 0.6616766452789307\n",
      "\n",
      "Average time per epoch: 89 seconds\n",
      "Final accuracy is 0.6616766452789307\n",
      "Difference between plain and encrypted accuracies: 0.04191619157791138\n"
     ]
    }
   ],
   "source": [
    "eelr = EncryptedLR(LR(n_features))\n",
    "accuracy = eelr.plain_accuracy(x_test, y_test)\n",
    "print(f\"Accuracy at epoch #0 is {accuracy}\")\n",
    "\n",
    "times = []\n",
    "for epoch in range(EPOCHS):\n",
    "    eelr.encrypt(ctx_training)\n",
    "    \n",
    "    # if you want to keep an eye on the distribution to make sure\n",
    "    # the function approximation is still working fine\n",
    "    # WARNING: this operation is time consuming\n",
    "    # encrypted_out_distribution(eelr, enc_x_train)\n",
    "    \n",
    "    t_start = time()\n",
    "    for enc_x, enc_y in zip(enc_x_train, enc_y_train):\n",
    "        enc_out = eelr.forward(enc_x)\n",
    "        eelr.backward(enc_x, enc_out, enc_y)\n",
    "    eelr.update_parameters()\n",
    "    t_end = time()\n",
    "    times.append(t_end - t_start)\n",
    "    \n",
    "    eelr.decrypt()\n",
    "    accuracy = eelr.plain_accuracy(x_test, y_test)\n",
    "    print(f\"Accuracy at epoch #{epoch + 1} is {accuracy}\")\n",
    "\n",
    "\n",
    "print(f\"\\nAverage time per epoch: {int(sum(times) / len(times))} seconds\")\n",
    "print(f\"Final accuracy is {accuracy}\")\n",
    "\n",
    "diff_accuracy = plain_accuracy - accuracy\n",
    "print(f\"Difference between plain and encrypted accuracies: {diff_accuracy}\")\n",
    "if diff_accuracy < 0:\n",
    "    print(\"Oh! We got a better accuracy when training on encrypted data! The noise was on our side...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even after running this cell many times myself, I always feel the joy when I see it working on encrypted data, so I hope you are feeling this joy as well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations!!! - Time to Join the Community!\n",
    "\n",
    "Congratulations on completing this notebook tutorial! If you enjoyed this and would like to join the movement towards privacy preserving, decentralized ownership of AI and the AI supply chain (data), you can do so in the following ways!\n",
    "\n",
    "### Star TenSEAL on GitHub\n",
    "\n",
    "The easiest way to help our community is just by starring the Repos! This helps raise awareness of the cool tools we're building.\n",
    "\n",
    "- [Star TenSEAL](https://github.com/OpenMined/TenSEAL)\n",
    "\n",
    "### Join our Slack!\n",
    "\n",
    "The best way to keep up to date on the latest advancements is to join our community! You can do so by filling out the form at [http://slack.openmined.org](http://slack.openmined.org). #lib_tenseal and #code_tenseal are the main channels for the TenSEAL project.\n",
    "\n",
    "### Join our Team!\n",
    "\n",
    "If you're excited about what we are working on TenSEAL, and if you're interested to work on homomorphic encryption related use cases, you should definitely join us!\n",
    "\n",
    "[Apply to the crypto team!](https://docs.google.com/forms/d/1T6MJ21V1lb7aEr4ilZOTYQXzxXP6KbpLumZVmTZMSuY/edit)\n",
    "\n",
    "\n",
    "### Donate\n",
    "\n",
    "If you don't have time to contribute to our codebase, but would still like to lend support, you can also become a Backer on our Open Collective. All donations go toward our web hosting and other community expenses such as hackathons and meetups!\n",
    "\n",
    "[OpenMined's Open Collective Page](https://opencollective.com/openmined)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
